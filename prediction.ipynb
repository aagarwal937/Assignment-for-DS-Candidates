{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score,plot_confusion_matrix,classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.constraints import max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "standard_sclaer = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_length_mapping(lst):\n",
    "    mapping = {}\n",
    "    for elem in lst:\n",
    "        if elem in [\"n/a\", \"< 1 year\"]:\n",
    "            mapping[elem] = 0\n",
    "        else:\n",
    "            mapping[elem] = int(elem.split()[0].strip(\"+\"))\n",
    "        \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_mapping(lst):\n",
    "    mapping = {}\n",
    "    for elem in lst:\n",
    "        if elem in [\"Default\", \"Charged Off\", \"Late (31-120 days)\",\n",
    "                   \"Does not meet the credit policy. Status:Charged Off\"]:\n",
    "            mapping[elem] = 1\n",
    "        else:\n",
    "            mapping[elem] = 0\n",
    "    \n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the NULLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNulls(dataframe, axis =1, percent=0.3):\n",
    "    df = dataframe.copy()\n",
    "    ishape = df.shape\n",
    "    if axis == 0:\n",
    "        rownames = df.transpose().isnull().sum()\n",
    "        rownames = list(rownames[rownames.values > percent*len(df)].index)\n",
    "        df.drop(df.index[rownames],inplace=True) \n",
    "        print(\"nNumber of Rows droppedt: \",len(rownames))\n",
    "    else:\n",
    "        colnames = (df.isnull().sum()/len(df))\n",
    "        colnames = list(colnames[colnames.values>=percent].index)\n",
    "        df.drop(labels = colnames,axis =1,inplace=True)        \n",
    "        print(\"Number of Columns droppedt: \",len(colnames))\n",
    "    print(\"nOld dataset rows,columns\",ishape,\"nNew dataset rows,columns\",df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = \"/Users/mlabhishek/Documents/Assignment for DS Candidates\"\n",
    "data = pandas.read_csv(r\"loan_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NULL values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove columns where NA values are more than or equal to 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = removeNulls(data, axis =1,percent = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove those columns where more than 1% of the rows for that column contain a null value.\n",
    "data = data[[label for label in data if data[label].isnull().sum() <= 0.01 * data.shape[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all columns with only one unique value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = data.nunique()\n",
    "unique = unique[unique.values == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(labels = list(unique.index), axis =1, inplace=True)\n",
    "print(\"New shape of the data is :\", data.shape , \"rows & columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have to do something with those NULL values. We can:\n",
    "    ** remove rows cointain NULL values, **\n",
    "    ** fill them with median or mode value, **\n",
    "    ** or use some imputation and try to predict their missing values. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.shape[0] / data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the non useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"id\", \"member_id\", \"sub_grade\", \"url\", \"zip_code\", \"title\"], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with only one value\n",
    "for label in list(data):\n",
    "    if len(data[label].unique()) < 5:\n",
    "        print(data[label].value_counts())\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # We can see that feature \"pymnt_plan\" has only two possible values: \"n\" and \"y\", but with only 10 occurrences of \"y\" (less than 1%), so definitely it is insignificant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"pymnt_plan\"], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,30))\n",
    "seaborn.heatmap(data.corr(),annot=True)\n",
    "plt.title('Correlation Matrix (for Loan Status)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target column\n",
    "list(data[\"loan_status\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our goal is to prepare predictive model of default. Default client is defined as one with loan_status variable taking on the following levels: 'Charged Off', 'Default', 'Late (31-120 days)', 'Does not meet the credit policy. Status:Charged Off'. So this values we will define as ones and the rest as zeros.\n",
    "data[\"loan_status\"] = data[\"loan_status\"].map(target_mapping(data[\"loan_status\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do something with our non-numerical features!\n",
    "data.select_dtypes(include=[\"object\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"initial_list_status\"] = data[\"initial_list_status\"].map({\"f\": 1, \"w\": 0})\n",
    "data[\"term\"] = data[\"term\"].apply(str).str.split().str[0].astype(\"int\")\n",
    "data[\"last_pymnt_amnt\"] = data[\"last_pymnt_amnt\"].astype(\"float\")\n",
    "data = pandas.get_dummies(data, columns=list(data.select_dtypes(include=[\"object\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all continuous variables to numeric values.\n",
    "numeric_columns = ['loan_amnt','funded_amnt','funded_amnt_inv','installment','int_rate','annual_inc','dti']\n",
    "data[numeric_columns] = data[numeric_columns].apply(pandas.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numeric_columns] = data[numeric_columns].apply(pandas.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loan Status: Remove all records with a value of less than 1.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.loan_status.value_counts()*100)/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_loan_status = (data.loan_status.value_counts()*100)/len(data)\n",
    "del_loan_status = del_loan_status[(del_loan_status < 1.5)]\n",
    "data.drop(labels = data[data.loan_status.isin(del_loan_status.index)].index, inplace=True)\n",
    "print(\"So now we are left with\",data.shape ,\"rows & columns.\")\n",
    "print(data.loan_status.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('loan_status', axis=1)\n",
    "y = data['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaler is MinMax scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING A KNN CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = KNN.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING THE XGBOOST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model to training data\n",
    "XG_BOOST = XGBClassifier()\n",
    "XG_BOOST.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = XG_BOOST.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(XG_BOOST,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING TNE RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_FOREST = RandomForestClassifier(n_estimators=100)\n",
    "RANDOM_FOREST.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = RANDOM_FOREST.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,preds))\n",
    "plot_confusion_matrix(RANDOM_FOREST,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN = Sequential()\n",
    "\n",
    "# input layer\n",
    "ANN.add(Dense(119,  activation='relu'))\n",
    "ANN.add(Dropout(0.1))\n",
    "\n",
    "# hidden layer\n",
    "ANN.add(Dense(78,  activation='relu'))\n",
    "ANN.add(Dropout(0.1))\n",
    "\n",
    "# hidden layer\n",
    "ANN.add(Dense(39, activation='relu'))\n",
    "ANN.add(Dropout(0.1))\n",
    "\n",
    "# hidden layer\n",
    "ANN.add(Dense(19, activation='relu'))\n",
    "ANN.add(Dropout(0.1))\n",
    "\n",
    "# output layer\n",
    "ANN.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "ANN.compile(loss='binary_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=10,\n",
    "          batch_size=256,\n",
    "          validation_data=(X_test, y_test), \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pandas.DataFrame(ANN.history.history)\n",
    "losses[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (ANN.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,predictions)\n",
    "plot = sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', square=True) #plot_confusion_matrix does not work directly for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (ANN.predict(X_test) > 0.7).astype(\"int32\")\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "526c4d04d929160fed228ea715f9cc845b199e8f58c2ca2a543ce0c1724723e1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
